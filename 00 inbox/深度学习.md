# lesson 1-Introduction of Deeping Learning 

*Machine Learning ≈ Looking for Function*
- 这里的意思就是，机器学习简单来说就是找到一个函数：
   比如下面举例子，我们要通过一个函数转换一段声音，或者说要判断一个图片的内容，或者说要判断下围棋的位置，这都是一个函数，明显的我们是不能直接写出来这个函数，要通过机器的方式。
  ![[../00 attachment/Pasted image 20250222202428.png|700]]

## 机器学习任务分类

![[../00 attachment/Pasted image 20250222202838.png|800]]

1. *Regression(统计，回归)*：我们提供给函数多个相关的数据，函数输出的是一个数值。
2. *Classification(分类)*：我们提供多个选项（不固定），函数的任务就是从中选择出一个符合的结果。如：上面的 f 函数的任务就是判断一封邮件是不是垃圾邮件，只需要输出 YES/NO。
3. *Strutured Learning:(结构化学习)* ：这也是机器学习的一大任务，他的任务就是创造一个有结构的东西，比如：写一篇文章

## 机器怎么学习 (Train 阶段)

- 简介：这里李宏毅的课程以自己 Youtube 的每天用户点阅人数作为例子，要找一个函数，函数的作用是根据以往的点阅人数去预测第二天的点阅人数。

### 1. 写出一个带有未知参数的函数

![[../00 attachment/Pasted image 20250222204733.png|800]]

- 这是机器学习的第一步，写出一个带有未知参数的函数，这个函数我们凭借的是自己对这个问题的了解，对相应现象的认识，以及 domain knowledge(领域知识)。
  1. 我们称写出的未知函数为*model*; 称未来的订阅人数 y 和 前一天的订阅人数 x 为 *feature*; 称 w 为 *weight*; 称 b 为*bias*；

### 2. 定义 Loss 函数

![[../00 attachment/Pasted image 20250222210833.png|800]]

- *Loss 函数* 是一个关于未知参数*w 和 b*的函数,他的作用是判断我们输入的参数好不好。Loss（b,w）通过已有的数据来判断我们的参数好不好。比如上图中：我们的 Loss 函数是*Loss（0.5k，1）*，也就是*y=0.5k+x<sub>1</sub>*, 这样的话我们根据这个函数可以计算过去三年每一天的预测结果，然后与实际值对比就可以计算出来误差。这里我们称正确数值为*label*.

![[../00 attachment/Pasted image 20250222211553.png|800]]

- 我们计算出来已有数据的所有的差 *e<sub>n</sub>* 后，就可以计算出来平均的误差，那么我们就知道了我们的参数好还是不好；这种计算*L*的方式叫做*MAE（mean absolute error）*;
- 此外我们还有另一种方式，叫做*MSE(mean square error)*,选择哪一种方式看问题的需要。

*Error Surface*: 我们由不同的参数得到的效果画出来的等高线图，

![[../00 attachment/Pasted image 20250222212144.png|800]]

## 3. Optimization（/ˌɒptɪmaɪˈzeɪʃən/，最优化）-*Gradient Descent(梯度下降)*

![[../00 attachment/Pasted image 20250222212849.png|800]]

- 机器学习的第三步就是找到一组最好的 w 和 b ,使得我们的 L 最小；我们称之为 w$^*$ 和 b$^*$ ; 那么如何找到 w$^*$ 和 b$^*$ 呢？我们使用的方法叫做 *Gradient Descent(梯度下降)*
  1. 我们首先假设只有一个未知的参数，也就是 w,那么我们可以得到 Loss 关于 w 的函数，如上图所示；
  2. 我们随机的提出一个 w$^0$ ,(这我们初始学习阶段认为 w$^0$ 是随机确定的，以后可能 w$^0$ 对于特定的任务有自己的确定方法)，然后我们求在 w$^0$ 处的函数 Loss 的微分，然后向低的方向移动。这时候问题就来了，我们移动是要移动多少呢？
   
![[../00 attachment/Pasted image 20250222221445.png|800]]

*学习率（learning rate）:* 我们决定 w$^0$ 移动的多少一方面根据 w$^0$ 处的斜率的大小，另一方面我们根据学习率，学习率是我们认为设置的一个参数，像这种需要我们自己设定的东西，称之为*超参数（hyperparrameters）*。

**问题：这里有一个问题，在上面的图中 Loss 函数的数值出现了负值，但是我们上面学习到的误差函数实际上是通过绝对值进行的计算，那怎么会有负值呢？**

- 这里我们说 Loss 函数完全是由人为设计的，因此不一定是绝对值的形式，这里我们上图中使用到的 Loss 函数仅仅是作为一个例子使用，而不是与之前对应的由绝对值计算出来的 Loss 函数。
  
  ![[../00 attachment/Pasted image 20250223145926.png|800]]

我们梯度下降（Gradient Descent）什么时候停止呢？一般有两种情况，一是到达了设定的更新次数，比如说一百万次，那么他就进行一百万次的更新；二是到达了附近的极小值点；如上图 W$^T$ 这里就是到达了极小值点，那么问题就来了，很明显的这是极小值，不一定是最小值。我们称这个到达的极小值叫做*Local minima(局部最小值)*，称最优解那里为*Global minima(全局最小值)*。
**课程中李宏毅老师说这里的 Local minima 问题实际上不是一个痛点，梯度下降（Gradient Descent）的痛点在另一方面！**

我们知道了一个参数如何计算，那么两个参数也是一样的操作，随机取得两个参数，然后进行 Gradient Descent.

![[../00 attachment/Pasted image 20250223151334.png|800]]

## 进一步模型的改进-Sigmoid Function

![[../00 attachment/Pasted image 20250223152033.png|800]]

我们成功得到函数之后，就可以用它来预测以后的订阅人数，然后我们就可以得到这样一张图表。由图表中我们观察，然后分析数据得知，这个数据每隔七天都好像存在一个循环，然后每次到礼拜五和礼拜六时候订阅人数都十分少，那么我们知道了每天的数据，可能涉及到的是前七天的数据，因此我们的函数只考虑了前一天的数据，这是一大缺陷。因此我们要对模型进行修改：

![[../00 attachment/Pasted image 20250223152602.png|800]]

如上图我们首先考虑前七天的数据作为相关因素，每一天的数据都有自己的权重参数。训练后可以知道，我们的模型更加注重前一天的数据，权重是 0.79。那么我们又考虑是否考虑更多天的数据，效果会不会更好呢，下面我们依次考虑了 28 天和 56 天的数据，确实得到了更好的效果，但是在对未来的预测方面，似乎效果得到了极限。我们称这样的模型为*Linear models(线性模型)*。

![[../00 attachment/Pasted image 20250223153317.png|800]]

这个时候问题又来了，很明显的我们知道，线性模型的局限性太大了，他只是一条直线，可以模拟的情况太有限，或者说使用它去模型实际情况误差太大。

![[../00 attachment/Pasted image 20250223154036.png|800]]

那么我们观察*red curve*，好像发现，红色的曲折的线段可以由一个常数 + 一个特殊的函数。那么如何构造呢？首先我们确定一个常数，也就是 red curve 与 y 轴的交点的数值。然后蓝色函数的起始部分都设置为 0，我们用他的折线部分去拟合*red curve*，在每两个转折点中间设置蓝色曲线的斜率和这一部分的 red curve 斜率相同即可成功模拟这一部分，如图所示：*1 号折线帮助模拟了第一部分的 red curve,2 号曲线帮助模拟了第二部分的 red curve,三号曲线帮助模拟了第三部分。* 

![[../00 attachment/Pasted image 20250223154651.png|800]]

所以说只要我们使用足够多的蓝色折线，就可以模拟更加复杂的*分段线性（Piecewise Linear）* 的函数。那么问题又来了，曲线怎么办呢？实际上没有关系，我们对于曲线的处理是，在曲线上取几个特殊的点，把它们连接起来，就构成了*Piecewise Linear curve*,只要我们取到的点足够多，那么我们模拟的曲线也就更加准确。

![[../00 attachment/Pasted image 20250223154925.png|800]]

那么这么方便的蓝色折线我们又该怎么写出来呢？实际上我们使用一个函数去逼近这个折线型的曲线。我们称之为*Sigmoid Function(S 型曲线/乙型曲线)*。相应地我们称上面的折线型为*Hard Sigmoid*.我们只需要改变相应的*b,w,c的数值*，就可以得到我们想要的hard function.

![[../00 attachment/Pasted image 20250223155538.png|800]]

![[../00 attachment/Pasted image 20250223155748.png|800]]

这样的话我们就得到了新的模型。这里我们看到，我们如果只考虑一天的因素*wx<sub>1</sub>* 的话，我们的模型改进后就是第二行的函数，一个偏置项b+几个Sigmoid 函数。如果我们考虑很多天的因素的话 ->  sigmoid函数的参数w就会不一样，而SIgmoid函数的个数还是一样的。

![[../00 attachment/Pasted image 20250223160401.png|800]]