# lesson 1-Introduction of Deeping Learning 

*Machine Learning ≈ Looking for Function*
- 这里的意思就是，机器学习简单来说就是找到一个函数：
   比如下面举例子，我们要通过一个函数转换一段声音，或者说要判断一个图片的内容，或者说要判断下围棋的位置，这都是一个函数，明显的我们是不能直接写出来这个函数，要通过机器的方式。
  ![[../00 attachment/Pasted image 20250222202428.png|700]]

## 机器学习任务分类

![[../00 attachment/Pasted image 20250222202838.png|800]]

1. *Regression(统计，回归)*：我们提供给函数多个相关的数据，函数输出的是一个数值。
2. *Classification(分类)*：我们提供多个选项（不固定），函数的任务就是从中选择出一个符合的结果。如：上面的 f 函数的任务就是判断一封邮件是不是垃圾邮件，只需要输出 YES/NO。
3. *Strutured Learning:(结构化学习)* ：这也是机器学习的一大任务，他的任务就是创造一个有结构的东西，比如：写一篇文章

## 机器怎么学习 (Train 阶段)

- 简介：这里李宏毅的课程以自己 Youtube 的每天用户点阅人数作为例子，要找一个函数，函数的作用是根据以往的点阅人数去预测第二天的点阅人数。

### 1. 写出一个带有未知参数的函数

![[../00 attachment/Pasted image 20250222204733.png|800]]

- 这是机器学习的第一步，写出一个带有未知参数的函数，这个函数我们凭借的是自己对这个问题的了解，对相应现象的认识，以及 domain knowledge(领域知识)。
  1. 我们称写出的未知函数为*model*; 称未来的订阅人数 y 和 前一天的订阅人数 x 为 *feature*; 称 w 为 *weight*; 称 b 为*bias*；

### 2. 定义 Loss 函数

![[../00 attachment/Pasted image 20250222210833.png|800]]

- *Loss 函数* 是一个关于未知参数*w 和 b*的函数,他的作用是判断我们输入的参数好不好。Loss（b,w）通过已有的数据来判断我们的参数好不好。比如上图中：我们的 Loss 函数是*Loss（0.5k，1）*，也就是*y=0.5k+x<sub>1</sub>*, 这样的话我们根据这个函数可以计算过去三年每一天的预测结果，然后与实际值对比就可以计算出来误差。这里我们称正确数值为*label*.

![[../00 attachment/Pasted image 20250222211553.png|800]]

- 我们计算出来已有数据的所有的差 *e<sub>n</sub>* 后，就可以计算出来平均的误差，那么我们就知道了我们的参数好还是不好；这种计算*L*的方式叫做*MAE（mean absolute error）*;
- 此外我们还有另一种方式，叫做*MSE(mean square error)*,选择哪一种方式看问题的需要。

*Error Surface*: 我们由不同的参数得到的效果画出来的等高线图，

![[../00 attachment/Pasted image 20250222212144.png|800]]

## 3. Optimization（/ˌɒptɪmaɪˈzeɪʃən/，最优化）-*Gradient Descent(梯度下降)*

![[../00 attachment/Pasted image 20250222212849.png|800]]

- 机器学习的第三步就是找到一组最好的 w 和 b ,使得我们的 L 最小；我们称之为 w$^*$ 和 b$^*$ ; 那么如何找到 w$^*$ 和 b$^*$ 呢？我们使用的方法叫做 *Gradient Descent(梯度下降)*
  1. 我们首先假设只有一个未知的参数，也就是 w,那么我们可以得到 Loss 关于 w 的函数，如上图所示；
  2. 我们随机的提出一个 w$^0$ ,(这我们初始学习阶段认为 w$^0$ 是随机确定的，以后可能 w$^0$ 对于特定的任务有自己的确定方法)，然后我们求在 w$^0$ 处的函数 Loss 的微分，然后向低的方向移动。这时候问题就来了，我们移动是要移动多少呢？
   
![[../00 attachment/Pasted image 20250222221445.png|800]]

*学习率（learning rate）:* 我们决定 w$^0$ 移动的多少一方面根据 w$^0$ 处的斜率的大小，另一方面我们根据学习率，学习率是我们认为设置的一个参数，像这种需要我们自己设定的东西，称之为*超参数（hyperparrameters）*。

**问题：这里有一个问题，在上面的图中 Loss 函数的数值出现了负值，但是我们上面学习到的误差函数实际上是通过绝对值进行的计算，那怎么会有负值呢？**

- 这里我们说 Loss 函数完全是由人为设计的，因此不一定是绝对值的形式，这里我们上图中使用到的 Loss 函数仅仅是作为一个例子使用，而不是与之前对应的由绝对值计算出来的 Loss 函数。
  
  ![[../00 attachment/Pasted image 20250223145926.png|800]]

我们梯度下降（Gradient Descent）什么时候停止呢？一般有两种情况，一是到达了设定的更新次数，比如说一百万次，那么他就进行一百万次的更新；二是到达了附近的极小值点；如上图 W$^T$ 这里就是到达了极小值点，那么问题就来了，很明显的这是极小值，不一定是最小值。我们称这个到达的极小值叫做*Local minima(局部最小值)*，称最优解那里为*Global minima(全局最小值)*。
**课程中李宏毅老师说这里的 Local minima 问题实际上不是一个痛点，梯度下降（Gradient Descent）的痛点在另一方面！**

我们知道了一个参数如何计算，那么两个参数也是一样的操作，随机取得两个参数，然后进行 Gradient Descent.

![[../00 attachment/Pasted image 20250223151334.png|800]]

## 进一步模型的改进

### 使用 -Sigmoid Function 对于第一步的改进

![[../00 attachment/Pasted image 20250223152033.png|800]]

我们成功得到函数之后，就可以用它来预测以后的订阅人数，然后我们就可以得到这样一张图表。由图表中我们观察，然后分析数据得知，这个数据每隔七天都好像存在一个循环，然后每次到礼拜五和礼拜六时候订阅人数都十分少，那么我们知道了每天的数据，可能涉及到的是前七天的数据，因此我们的函数只考虑了前一天的数据，这是一大缺陷。因此我们要对模型进行修改：

![[../00 attachment/Pasted image 20250223152602.png|800]]

如上图我们首先考虑前七天的数据作为相关因素，每一天的数据都有自己的权重参数。训练后可以知道，我们的模型更加注重前一天的数据，权重是 0.79。那么我们又考虑是否考虑更多天的数据，效果会不会更好呢，下面我们依次考虑了 28 天和 56 天的数据，确实得到了更好的效果，但是在对未来的预测方面，似乎效果得到了极限。我们称这样的模型为*Linear models(线性模型)*。

![[../00 attachment/Pasted image 20250223153317.png|800]]

这个时候问题又来了，很明显的我们知道，线性模型的局限性太大了，他只是一条直线，可以模拟的情况太有限，或者说使用它去模型实际情况误差太大。

![[../00 attachment/Pasted image 20250223154036.png|800]]

那么我们观察*red curve*，好像发现，红色的曲折的线段可以由一个常数 + 一个特殊的函数。那么如何构造呢？首先我们确定一个常数，也就是 red curve 与 y 轴的交点的数值。然后蓝色函数的起始部分都设置为 0，我们用他的折线部分去拟合*red curve*，在每两个转折点中间设置蓝色曲线的斜率和这一部分的 red curve 斜率相同即可成功模拟这一部分，如图所示：*1 号折线帮助模拟了第一部分的 red curve,2 号曲线帮助模拟了第二部分的 red curve,三号曲线帮助模拟了第三部分。* 

![[../00 attachment/Pasted image 20250223154651.png|800]]

所以说只要我们使用足够多的蓝色折线，就可以模拟更加复杂的*分段线性（Piecewise Linear）* 的函数。那么问题又来了，曲线怎么办呢？实际上没有关系，我们对于曲线的处理是，在曲线上取几个特殊的点，把它们连接起来，就构成了*Piecewise Linear curve*,只要我们取到的点足够多，那么我们模拟的曲线也就更加准确。

![[../00 attachment/Pasted image 20250223154925.png|800]]

那么这么方便的蓝色折线我们又该怎么写出来呢？实际上我们使用一个函数去逼近这个折线型的曲线。我们称之为*Sigmoid Function(S 型曲线/乙型曲线)*。相应地我们称上面的折线型为*Hard Sigmoid*.我们只需要改变相应的*b,w,c 的数值*，就可以得到我们想要的 hard function.

![[../00 attachment/Pasted image 20250223155538.png|800]]

![[../00 attachment/Pasted image 20250223155748.png|800]]

这样的话我们就得到了新的模型。这里我们看到，我们如果只考虑一天的因素*wx<sub>1</sub>* 的话，我们的模型改进后就是第二行的函数，一个偏置项 b+ 几个 Sigmoid 函数。如果我们考虑很多天的因素的话 -> sigmoid 函数的参数 w 就会不一样，而 SIgmoid 函数的个数还是一样的。

![[../00 attachment/Pasted image 20250223160401.png|800]]

我们假设我们的 model 考虑前三天的数据，此外也假设我们的函数使用了三个 Sigmoid 函数去模拟实际的情况。那么对于每一个 Sigmoid 函数，他们是专门用来模拟实际 Piecewise Linear 的每一段的，那么对于每一个 sigmoid,他们都有自己的 *w<sub>ij</sub>* ,这里的 ij 代表第 *i* 个 Sigmoid 函数对第 *j* 个数据 *x<sub>j</sub>* 的权重。

![[../00 attachment/Pasted image 20250223163031.png|800]]

简化后实际上我们知道，这是明显的一个线性代数的向量相乘的形式。

![[../00 attachment/Pasted image 20250223163728.png|800]]

![[../00 attachment/Pasted image 20250223163906.png|800]]

可以知道这里*r<sub>i</sub>* 就是原本的线性函数，这里的 *a<sub>i</sub>* 就是将原来的线性函数转化为几个 sigmoid 函数的和 后的结果, *a* 向量是 几个 *a<sub>i</sub>* 构成的和向量。

![[../00 attachment/Pasted image 20250223164046.png|800]]

下面我们对未知的参数做一个总结：这里的两个 b 是不一样的，灰色的 b 是偏置项，绿色的 b 代表一个向量。这里我们把所有的未知参数拿出来组成一列的向量，称之为 $\theta$ 
那么到现在为止我们就重新的改进了我们机器学习的第一步，也就是写一个含有未知参数的 Function。
![[../00 attachment/Pasted image 20250223164318.png|800]]

### Optimization 的改进

- Loss 函数无需改进，计算方法和之前是完全一样的。
- 实际上 Optimization 的操作和之前也是一样的，只不过这里看起来复杂一些。我们是把所有参数集合而成的 $\theta向量$ 进行梯度下降的求解，这里求得的结果我们称作 *gradient*,也就是 *g = ▽ L（ $\theta$$^0$）* ,这里的▽表示的是求微分的操作。我们得到 gradient 后，这里的 gradient 也就是我们所有的未知参数构成的向量 求微分一次的结果。然后我们不断地求微分，不断的更新。

![[../00 attachment/Pasted image 20250223185946.png|800]]

#### epoch & batch

*batch*: 我们随机的把数据分成不同的部分，每一部分叫做一个 batch。每个 batch 数量不固定，随机分。

实际上我们训练 model, 更新 $\theta$ 并不是用所有的数据统一的去训练，而是把数分为不同的 batch，依次用每一个 batch 的数据去更新一次 $\theta$ ,我们所有的 batch 都对 $\theta$ 完成一次更新后，我们称之为一个*epoch*。
由此我们知道，当进行一个 epoch 后，参数更新了很多次。比如说，我们一共有 1000 个案例，每一个 batch 都被分了 10 个案例，那么我们进行一个 epoch 后，参数就进行了 100 的更新。 

![[../00 attachment/Pasted image 20250223191705.png|800]]

### ReLU (Rectified Linear Unit) 

像下图中的图形叫做 ReLu 函数，实际上我们可以使用两个 ReLu 函数合成一个 hard sigmoid 函数。

![[../00 attachment/Pasted image 20250223193128.png|800]]

*Activation function(激活函数)*：我们称 sigmoid 和 ReLU 为激活函数。
![[../00 attachment/Pasted image 20250223193341.png|500]]

由此我们可以对训练得到的 *a* 向量作为 *x 向量* 再进行同样的训练，训练的次数由我们自己决定，这也是一个超参数。 

![[../00 attachment/Pasted image 20250223193845.png|800]]

比如说我们训练了三层，每一层使用了100个ReLU激活函数，下面是得到的效果。

![[../00 attachment/Pasted image 20250223194850.png|800]]


![[../00 attachment/Pasted image 20250223195441.png]]